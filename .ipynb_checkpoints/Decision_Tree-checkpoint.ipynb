{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742f0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e9869f",
   "metadata": {},
   "source": [
    "Our decision tree algorithm is adapted from CART (Classification and Regression Trees) which is commonly used and is the algorithm implemented by Scikit Learn. Though our algorithm has the functionality to do regression tasks, we focus on its classification abilities and assume that the target variable $y$ has $2$ (for binary classification) or $k$ (for multi-class classification) categories. Starting from the root node where we have the training data of dimension $n * (l+1)$ consisting of feature vectors $x_i \\in R^l ,\\;i \\in 1,...,n$ and a label vector $y \\in R^n$, the decision tree finds the best feature and its best value that result in the lowest loss given a loss function to be minimized; and it recursively partitions the data into two branches or nodes. Using Scikit Learn notation, denote the data at node m as $Q_m$ with $N_m$ samples. The algorithm contains the following steps:\n",
    "\n",
    "1. At the node, find all possible splits for all attributes. For each potential split $\\theta = (j,t_m)$ consisting of a feature $j$ and threshold $t_m$, partition the data into two subsets $Q_{m}^{left}(\\theta)$ and $Q_{m}^{right}(\\theta)$ such that:\n",
    "\\begin{align*}\\label{eq:pareto mle2}\n",
    "&Q_{m}^{left}(\\theta) = \\left\\{(x,y)|x_j <= t_m\\right\\}   \\\\\n",
    "&Q_{m}^{right}(\\theta) = Q_m \\setminus Q_{m}^{left}(\\theta) \n",
    "\\end{align*}  \n",
    "\n",
    "2. Consider all potential splits by calculating the loss function $H$ and select the parameters $\\theta^*$ that minimize:\n",
    "$$ \\frac{N_{m}^{left}}{N_m}H(Q_{m}^{left}(\\theta)) +  \\frac{N_{m}^{right}}{N_m}H(Q_{m}^{right}(\\theta)) \n",
    "$$\n",
    "\n",
    "3. Create two child nodes $Q_{m}^{left}(\\theta^*)$ and $Q_{m}^{right}(\\theta^*)$ and repeat from Step 1 for each node until certain stopping criterion is satisfied. \n",
    "\\end{enumerate}\n",
    "\\\\ \\\\\n",
    "Some common stopping criteria in Step 3 includes: \\textbf{data\\_is\\_pure}, all labels in the tree are same; \\textbf{max\\_depth}, the maximum depth the tree can grow; \\textbf{min\\_samples\\_split}, the minimum samples required for considering a split.\n",
    "\n",
    "\n",
    "For classification tasks, two of the most widely used loss functions $H$ are Gini (1) and Entropy (2):\n",
    "\n",
    "\\begin{equation}\n",
    "H(Q_m) = \\sum_{k}p_{mk}(1-p_{mk}) \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "H(Q_m) = -\\sum_{k}p_{mk}log_{2}(p_{mk}) \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $p_{mk} = \\frac{1}{N_m}\\sum_{y \\in Q_m}{I(y=k)}$ and $k$ is the total number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b535a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, loss_func='gini'):\n",
    "    # supports three loss functions:\n",
    "    # gini and entropy for classification jobs,\n",
    "    # and mse for regression jobs\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    prop = counts / np.sum(counts)\n",
    "    if loss_func == 'entropy':\n",
    "        return np.float64(-np.dot(prop, np.log2(prop)))\n",
    "    elif loss_func == 'gini':\n",
    "        return np.float64(np.dot(prop, 1 - prop))\n",
    "    elif loss_func == 'mse':\n",
    "        return np.float64(np.sum(y - np.mean(y))**2 / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f1fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    The object that holds info of a node \n",
    "    Decision trees are made of node and edges.\n",
    "    Here egdes are simply whether feature value <= split value or not. \n",
    "    \n",
    "    Note that This is a recursive object. so it itself has left node object and right node object\n",
    "    '''\n",
    "\n",
    "    def __init__(self, indices, parent=None):\n",
    "        self.indices = indices  # observations idxs (data) associated with the node\n",
    "        self.left = None  # its left child node\n",
    "        self.right = None  # its right child node\n",
    "        self.split_feature = None  # the feature idx associated with this node for splitting to left and right childs\n",
    "        self.split_value = None  # the feature value for split\n",
    "\n",
    "        if parent:  # This applies to all nodes except the root node that we will initiate with.\n",
    "            self.depth = parent.depth + 1\n",
    "            self.X = parent.X  # inherit X from its parent node\n",
    "            self.y = parent.y  # inherit the target from its parent node\n",
    "            self.label = stats.mode(self.y[indices])[0][\n",
    "                0]  # stores the mode of the targets in a node (for classification only)\n",
    "            self.mean = np.mean(\n",
    "                self.y[indices]\n",
    "            )  # stores the mean of targets in a node (for regression only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478aad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(node, loss_func='gini', max_features=None):\n",
    "    '''\n",
    "    This function searches for only one feature at a time, and uses that best feature.\n",
    "    Thus it is greedy and does not consider the global picture, only results in local optimum.\n",
    "    '''\n",
    "    _, n_col = node.X.shape  # need the number of features of X\n",
    "    # randomly select features\n",
    "    feature_idx = np.random.choice(\n",
    "        n_col, np.minimum(n_col, max_features),\n",
    "        replace=False) if max_features else np.arange(0, n_col)\n",
    "    # initialize the loss\n",
    "    best_loss = np.inf\n",
    "    best_col, best_val = None, None\n",
    "    for i in feature_idx:  # iterate through each feature\n",
    "        X_i = node.X[node.indices, i]  # the column of X holding the feature\n",
    "        unique_values = np.unique(X_i)  # get all unique values of this feature\n",
    "        for val in unique_values:  # search for the split value that results in minimum loss\n",
    "            idx_left = node.indices[X_i <= val]\n",
    "            idx_right = node.indices[X_i > val]\n",
    "            if len(idx_left) == 0 or len(\n",
    "                    idx_right\n",
    "            ) == 0:  # if cannot split, skip the current loop and return loss = np.inf\n",
    "                continue\n",
    "            loss_left = loss(node.y[idx_left],\n",
    "                             loss_func=loss_func)  # loss of left node\n",
    "            loss_right = loss(node.y[idx_right],\n",
    "                              loss_func=loss_func)  # loss of right node\n",
    "            len_left, len_right = idx_left.shape[0], idx_right.shape[\n",
    "                0]  # number of instances in left and right\n",
    "            loss_total = (len_left * loss_left + len_right * loss_right) / (\n",
    "                len_left + len_right)  # weighted loss\n",
    "            if loss_total < best_loss:  # update step\n",
    "                best_loss = loss_total\n",
    "                best_col = i\n",
    "                best_val = val\n",
    "    return best_loss, best_col, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef00dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    '''\n",
    "    The object that holds the decision tree algorithm\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 loss_func='gini',\n",
    "                 min_samples_split=5,\n",
    "                 max_depth=10,\n",
    "                 max_features=None,\n",
    "                 random_state=None):\n",
    "        self.loss_func = loss_func  # criterion used for measuring loss\n",
    "        self.min_samples_split = min_samples_split  # int The minimum number of samples required to split an internal node\n",
    "        self.max_depth = max_depth  # int The maximum depth the tree can grow\n",
    "        self.max_features = max_features  # the maximum features to consider at each split\n",
    "        self.random_state = random_state  # control the randomness when growing the tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X  # Get X\n",
    "        self.y = y  # Get y\n",
    "        self.root = Node(\n",
    "            np.arange(0, X.shape[0]), None\n",
    "        )  # initiate the root, which has all indices and has no parent node\n",
    "        self.root.depth = 0  # initiate the root depth to 0\n",
    "        self.root.X = X  # initiate the root with all data\n",
    "        self.root.y = y  # initiate the root taget labels\n",
    "        self._grow_tree(self.root)  # Grow the tree from the root recursively\n",
    "        return self\n",
    "\n",
    "    def _grow_tree(self, node):\n",
    "        if len(np.unique(node.y)) == 1 or node.depth == self.max_depth or len(\n",
    "                node.indices) <= self.min_samples_split:\n",
    "            return  ## Stopping criteria\n",
    "        np.random.seed(self.random_state)  # set seed\n",
    "        cost, split_feature, split_value = greedy_search(\n",
    "            node, self.loss_func, self.max_features)  # search for best split\n",
    "        if np.isinf(\n",
    "                cost\n",
    "        ):  # return the leaf node when best_loss = np.inf i.e., no more split\n",
    "            return\n",
    "        test = node.X[\n",
    "            node.indices,\n",
    "            split_feature] <= split_value  # True or False that our data <= split value\n",
    "        node.split_feature = split_feature  # stores the best feature's idx\n",
    "        node.split_value = split_value  # stores the best feature's threshold\n",
    "        left = Node(node.indices[test], node)  # initiate its left child node\n",
    "        right = Node(node.indices[np.logical_not(test)],\n",
    "                     node)  # initiate its right child node\n",
    "\n",
    "        node.left = left  # stores the left child\n",
    "        node.right = right  # stores the right child\n",
    "        self._grow_tree(left)  # grows its left child\n",
    "        self._grow_tree(right)  # grows its right child\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.empty(X.shape[0])  # initialize the prediction array\n",
    "        for i, x in enumerate(X):\n",
    "            node = self.root\n",
    "            #loop along the dept of the tree looking region where the present data sample fall in based on the split feature and value\n",
    "            while node.left:  # while the left is not None, i.e., not leaf node\n",
    "                if x[node.\n",
    "                     split_feature] <= node.split_value:  # if the left condition holds\n",
    "                    node = node.left  # move to its left child\n",
    "                else:\n",
    "                    node = node.right  # moves to right child\n",
    "            # the loop terminates when you reach a leaf of the tree and the class probability of that node is taken for prediction\n",
    "            y_pred[i] = node.mean if self.loss_func == 'MSE' else node.label\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8f9b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7985074626865671\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('titanic.csv',\n",
    "                     usecols=[\n",
    "                         'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch',\n",
    "                         'Fare', 'Embarked'\n",
    "                     ])\n",
    "    df.isna().sum()\n",
    "\n",
    "    y = df['Survived']\n",
    "    X = df.drop('Survived', axis=1)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.3,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    ### Do some imputation since algorithms cannot handle missing values\n",
    "\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imp = SimpleImputer(strategy='mean')\n",
    "    X_train[['Age']] = imp.fit_transform(X_train[['Age']])\n",
    "\n",
    "    imp2 = SimpleImputer(strategy='most_frequent')\n",
    "    X_train[['Embarked']] = imp2.fit_transform(X_train[['Embarked']])\n",
    "\n",
    "    # Prepare train and test data\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import make_column_transformer\n",
    "    ohe = OneHotEncoder()\n",
    "    ct = make_column_transformer((ohe, ['Sex', 'Embarked']),\n",
    "                                 remainder='passthrough')\n",
    "    X_train_vect = ct.fit_transform(X_train)\n",
    "    y_train = y_train.values\n",
    "\n",
    "    X_test[['Age']] = imp.transform(X_test[['Age']])\n",
    "    X_test[['Embarked']] = imp2.transform(X_test[['Embarked']])\n",
    "    X_test_vect = ct.transform(X_test)\n",
    "    y_test = y_test.values\n",
    "    model = DecisionTree(loss_func='entropy',\n",
    "                         min_samples_split=30,\n",
    "                         max_depth=4,\n",
    "                         max_features=7,\n",
    "                         random_state=4)\n",
    "    model.fit(X_train_vect, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_vect)\n",
    "\n",
    "    accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bad10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8859649122807017\n",
      "9.826104879379272 s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Imports\n",
    "    from time import time\n",
    "    t0 = time()\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    data = datasets.load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=1234)\n",
    "\n",
    "    clf = DecisionTree(max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    t1 = time()\n",
    "\n",
    "    total = t1 - t0\n",
    "    print(total, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cacda49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.964200019836426 s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "clf = DecisionTree(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "t1 = time()\n",
    "\n",
    "total = t1 - t0\n",
    "print(total, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b535e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8947368421052632\n",
      "0.027340173721313477 s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Imports\n",
    "    from time import time\n",
    "    t0 = time()\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    data = datasets.load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=1234)\n",
    "\n",
    "    clf = DecisionTreeClassifier(max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    t1 = time()\n",
    "\n",
    "    total = t1 - t0\n",
    "    print(total, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81f9e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38e27fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006170749664306641 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "clf = DecisionTreeClassifier(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "t1 = time()\n",
    "total = t1 - t0\n",
    "print(total, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d19596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 7749.943820224719\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Imports\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    data = datasets.load_diabetes()\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=1234)\n",
    "\n",
    "    clf = DecisionTree(loss_func='mse')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(\"MSE:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f89f5646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 6502.460674157303\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = mean_squared_error(y_test, y_pred)\n",
    "    print(\"MSE:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a78021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76d7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
